"""
ä½¿ç”¨ Agent åœ¨ HumanEval ä¸Šè¿›è¡Œè¯„ä¼°
æµç¨‹ï¼šHumanEval é—®é¢˜ â†’ Agent ç”Ÿæˆä»£ç  â†’ å®˜æ–¹æµ‹è¯• â†’ ç»Ÿè®¡é€šè¿‡çŽ‡
"""
import json
import pathlib
from typing import Optional, List
import sys

import typer
from human_eval.data import read_problems, write_jsonl
from human_eval.evaluation import evaluate_functional_correctness
from human_eval.execution import check_correctness

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(pathlib.Path(__file__).parent.parent))

from agent.agent_loop import solve
from agent.llm_client import LLMClient
from benchmarks.humaneval_runner import _run_evaluation

app = typer.Typer(help="Evaluate Agent on HumanEval with standard tests.")


def _extract_python_code(text: str) -> str:
    """Extract the Python code block from LLM output, stripping prose/markdown."""
    text = text.strip()

    if "```" in text:
        parts = text.split("```")
        if len(parts) >= 2:
            code = parts[1]
            if code.startswith("python"):
                code = code[len("python"):].lstrip()
            return code.strip()

    lines = text.splitlines()
    for i, line in enumerate(lines):
        if line.strip().startswith(("def ", "class ", "import ", "from ")):
            return "\n".join(lines[i:]).strip()

    return text


def _generate_solutions(
    num_problems: Optional[int] = None,
    num_samples_per_task: int = 1,
    model_name: str = "qwen2.5-coder:7b",
    output_file: pathlib.Path = pathlib.Path("results/humaneval_samples.jsonl"),
) -> None:
    """
    ä½¿ç”¨ Agent ä¸º HumanEval é—®é¢˜ç”Ÿæˆä»£ç 
    
    Args:
        num_problems: è¯„ä¼°çš„é—®é¢˜æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ 164 ä¸ªï¼‰
        num_samples_per_task: æ¯ä¸ªé—®é¢˜ç”Ÿæˆçš„ä»£ç ç‰ˆæœ¬æ•°ï¼ˆç”¨äºŽ pass@kï¼‰
        model_name: ä½¿ç”¨çš„æ¨¡åž‹
        output_file: è¾“å‡ºçš„ JSONL æ–‡ä»¶
    """
    problems = read_problems()
    problem_ids = sorted(problems.keys())
    
    if num_problems:
        problem_ids = problem_ids[:num_problems]
    
    typer.echo(f"ðŸŽ¯ å¼€å§‹ä¸º {len(problem_ids)} ä¸ª HumanEval é—®é¢˜ç”Ÿæˆä»£ç ...")
    typer.echo(f"   â€¢ æ¯ä¸ªé—®é¢˜ç”Ÿæˆ {num_samples_per_task} ä¸ªç‰ˆæœ¬ï¼ˆç”¨äºŽ pass@{num_samples_per_task}ï¼‰")
    typer.echo(f"   â€¢ æ¨¡åž‹: {model_name}")
    typer.echo(f"   â€¢ è¾“å‡º: {output_file}\n")
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    completed = 0
    skipped = 0
    
    with output_file.open("w", encoding="utf-8") as f:
        for idx, task_id in enumerate(problem_ids, 1):
            problem = problems[task_id]
            
            typer.echo(f"[{idx}/{len(problem_ids)}] {task_id}: {problem.get('entry_point', 'unknown')}")
            
            # æž„å»º promptï¼šé—®é¢˜æè¿° + å‡½æ•°ç­¾å
            prompt = f"""{problem['prompt']}"""
            
            try:
                # ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªç‰ˆæœ¬
                for sample_idx in range(num_samples_per_task):
                    # æé«˜æ¸©åº¦ä»¥èŽ·å¾—å¤šæ ·åŒ–çš„ä»£ç 
                    client = LLMClient(
                        model_name=model_name,
                        temperature=0.5 + (sample_idx * 0.2),  # é€æ¸å¢žåŠ æ¸©åº¦
                        max_tokens=2048
                    )
                    
                    completion_raw = client.generate(prompt, response_format="text")
                    completion = _extract_python_code(completion_raw)
                    
                    # å†™å…¥ JSONL
                    record = {
                        "task_id": task_id,
                        "completion": completion,
                        "completion_id": f"agent-{sample_idx}",
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
                
                completed += 1
                typer.echo(f"   âœ“ æˆåŠŸç”Ÿæˆ {num_samples_per_task} ä¸ªç‰ˆæœ¬\n")
                
            except Exception as e:
                skipped += 1
                typer.echo(f"   âœ— ç”Ÿæˆå¤±è´¥: {str(e)}\n")
    
    typer.echo(f"\n{'='*60}")
    typer.echo(f"ðŸ“ ç”Ÿæˆå®Œæˆ:")
    typer.echo(f"   â€¢ æˆåŠŸ: {completed}/{len(problem_ids)} ä¸ªé—®é¢˜")
    typer.echo(f"   â€¢ å¤±è´¥: {skipped} ä¸ªé—®é¢˜")
    typer.echo(f"   â€¢ æ€»æ ·æœ¬æ•°: {completed * num_samples_per_task}")
    typer.echo(f"   â€¢ è¾“å‡ºæ–‡ä»¶: {output_file}")
    typer.echo(f"{'='*60}\n")


def _evaluate_solutions(
    samples_file: pathlib.Path = pathlib.Path("results/humaneval_samples.jsonl"),
    output_file: pathlib.Path = pathlib.Path("results/humaneval_report.json"),
    k_values: Optional[List[int]] = None,
    n_workers: int = 4,
    timeout: float = 3.0,
) -> None:
    """
    ä½¿ç”¨å®˜æ–¹ HumanEval æµ‹è¯•è¯„ä¼°ç”Ÿæˆçš„ä»£ç 
    
    Args:
        samples_file: åŒ…å«ç”Ÿæˆä»£ç çš„ JSONL æ–‡ä»¶
        output_file: è¾“å‡ºçš„è¯„ä¼°æŠ¥å‘Š
        k_values: pass@k çš„ k å€¼åˆ—è¡¨
        n_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        timeout: å•ä¸ªæµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    """
    if k_values is None:
        k_values = [1]
    
    typer.echo(f"\n{'='*60}")
    typer.echo(f"ðŸ§ª å¼€å§‹ HumanEval è¯„ä¼°...")
    typer.echo(f"   â€¢ æ ·æœ¬æ–‡ä»¶: {samples_file}")
    typer.echo(f"   â€¢ è¯„ä¼°æŒ‡æ ‡: pass@{', pass@'.join(map(str, k_values))}")
    typer.echo(f"   â€¢ å¹¶è¡Œè¿›ç¨‹: {n_workers}")
    typer.echo(f"   â€¢ è¶…æ—¶æ—¶é—´: {timeout}s")
    typer.echo(f"{'='*60}\n")
    
    if not samples_file.exists():
        typer.echo(f"âŒ æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {samples_file}")
        raise typer.Exit(code=1)
    
    try:
        _run_evaluation(samples_file, output_file, k_values, n_workers, timeout, None)
        
        # è¯»å–å¹¶æ˜¾ç¤ºç»“æžœæ‘˜è¦
        if output_file.exists():
            with output_file.open("r", encoding="utf-8") as f:
                report = json.load(f)
            
            typer.echo(f"\n{'='*60}")
            typer.echo(f"ðŸ“Š è¯„ä¼°ç»“æžœæ‘˜è¦")
            typer.echo(f"{'='*60}")
            
            summary = report.get("summary", {})
            typer.echo(f"\nâœ“ è¯„ä¼°ä»»åŠ¡æ•°: {summary.get('num_tasks', 'N/A')}")
            typer.echo(f"âœ“ è¯„ä¼°æ ·æœ¬æ•°: {summary.get('num_samples', 'N/A')}")
            
            typer.echo(f"\nðŸ“ˆ é€šè¿‡çŽ‡:")
            pass_at_k = report.get("pass_at_k", {})
            for k_val in sorted(k_values):
                key = f"pass@{k_val}"
                rate = pass_at_k.get(key, "N/A")
                if isinstance(rate, float):
                    typer.echo(f"   â€¢ {key}: {rate*100:.2f}%")
                else:
                    typer.echo(f"   â€¢ {key}: {rate}")
            
            typer.echo(f"\nðŸ“ å®Œæ•´æŠ¥å‘Š: {output_file}")
            typer.echo(f"{'='*60}\n")
    
    except Exception as e:
        typer.echo(f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        raise typer.Exit(code=1)


@app.command()
def generate(
    num_problems: Optional[int] = typer.Option(
        None,
        help="è¯„ä¼°çš„é—®é¢˜æ•°ï¼ˆé»˜è®¤å…¨éƒ¨ 164 ä¸ªï¼‰ã€‚è¾“å…¥ 10 å¯åªè¯„ä¼°å‰ 10 ä¸ªã€‚"
    ),
    num_samples: int = typer.Option(
        1,
        "--samples",
        help="æ¯ä¸ªé—®é¢˜ç”Ÿæˆçš„ä»£ç ç‰ˆæœ¬æ•°ï¼ˆç”¨äºŽ pass@k è¯„ä¼°ï¼‰"
    ),
    model_name: str = typer.Option(
        "qwen2.5-coder:7b",
        "--model",
        help="ä½¿ç”¨çš„ LLM æ¨¡åž‹"
    ),
    output: pathlib.Path = typer.Option(
        pathlib.Path("results/humaneval_samples.jsonl"),
        help="è¾“å‡ºçš„æ ·æœ¬æ–‡ä»¶"
    ),
) -> None:
    """ä¸º HumanEval é—®é¢˜ç”Ÿæˆä»£ç """
    _generate_solutions(num_problems, num_samples, model_name, output)


@app.command()
def evaluate(
    samples: pathlib.Path = typer.Option(
        pathlib.Path("results/humaneval_samples.jsonl"),
        help="åŒ…å«ç”Ÿæˆä»£ç çš„ JSONL æ–‡ä»¶"
    ),
    output: pathlib.Path = typer.Option(
        pathlib.Path("results/humaneval_report.json"),
        help="è¾“å‡ºçš„è¯„ä¼°æŠ¥å‘Š"
    ),
    k: List[int] = typer.Option(
        [1],
        "--k",
        help="pass@k çš„ k å€¼ï¼ˆå¯é‡å¤ï¼Œå¦‚ --k 1 --k 10ï¼‰"
    ),
    n_workers: int = typer.Option(4, help="å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°"),
    timeout: float = typer.Option(3.0, help="å•ä¸ªæµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"),
) -> None:
    """ä½¿ç”¨å®˜æ–¹ HumanEval æµ‹è¯•è¯„ä¼°ä»£ç """
    _evaluate_solutions(samples, output, k, n_workers, timeout)


@app.command()
def run_all(
    num_problems: Optional[int] = typer.Option(
        None,
        help="è¯„ä¼°çš„é—®é¢˜æ•°ã€‚è¾“å…¥ 5 å¯åªè¯„ä¼°å‰ 5 ä¸ªã€‚"
    ),
    num_samples: int = typer.Option(
        1,
        "--samples",
        help="æ¯ä¸ªé—®é¢˜ç”Ÿæˆçš„ä»£ç ç‰ˆæœ¬æ•°"
    ),
    model_name: str = typer.Option(
        "qwen2.5-coder:7b",
        "--model",
        help="ä½¿ç”¨çš„ LLM æ¨¡åž‹"
    ),
    k: List[int] = typer.Option(
        [1],
        "--k",
        help="pass@k çš„ k å€¼"
    ),
    max_iterations: int = typer.Option(
        3,
        "--max-iterations",
        help="æœ€å¤§è°ƒè¯•è¿­ä»£æ¬¡æ•°"
    ),
    enable_plan: bool = typer.Option(
        False,
        "--enable-plan",
        help="æ˜¯å¦å¯ç”¨è§„åˆ’é˜¶æ®µï¼ˆå¤šä»»åŠ¡æ‹†è§£ï¼‰"
    ),
) -> None:
    """ä½¿ç”¨å®Œæ•´ Agent å·¥ä½œæµè¯„ä¼°ï¼ˆplanner â†’ coder â†’ tester â†’ debuggerï¼‰"""
    problems = read_problems()
    problem_ids = sorted(problems.keys())
    
    if num_problems:
        problem_ids = problem_ids[:num_problems]
    
    typer.echo(f"\nðŸŽ¯ å¼€å§‹ç”¨å®Œæ•´ Agent å·¥ä½œæµè¯„ä¼° {len(problem_ids)} ä¸ª HumanEval é—®é¢˜...")
    typer.echo(f"   â€¢ æ¯ä¸ªé—®é¢˜ç”Ÿæˆ {num_samples} ä¸ªç‰ˆæœ¬")
    typer.echo(f"   â€¢ æ¨¡åž‹: {model_name}")
    typer.echo(f"   â€¢ æœ€å¤§è°ƒè¯•æ¬¡æ•°: {max_iterations}")
    typer.echo(f"   â€¢ è§„åˆ’é˜¶æ®µ: {'å¯ç”¨' if enable_plan else 'ç¦ç”¨'}\n")
    
    samples_file = pathlib.Path("results/humaneval_samples.jsonl")
    samples_file.parent.mkdir(parents=True, exist_ok=True)
    
    completed = 0
    passed_count = 0
    all_samples = []
    
    with samples_file.open("w", encoding="utf-8") as f:
        for idx, task_id in enumerate(problem_ids, 1):
            problem = problems[task_id]
            entry_point = problem.get('entry_point', 'unknown')
            
            typer.echo(f"\n{'='*70}")
            typer.echo(f"[{idx}/{len(problem_ids)}] {task_id}: {entry_point}")
            typer.echo(f"{'='*70}")
            
            # æž„å»ºéœ€æ±‚ï¼šHumanEval çš„ prompt å°±æ˜¯éœ€æ±‚æè¿°
            requirement = problem['prompt']
            
            try:
                # ä¸ºè¿™ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªç‰ˆæœ¬
                task_passed = False
                last_error = "æœªè¯„ä¼°"
                last_completion = ""
                
                for sample_idx in range(num_samples):
                    typer.echo(f"\nðŸ¤– ç‰ˆæœ¬ {sample_idx+1}/{num_samples} - å¯åŠ¨ Agent å·¥ä½œæµ...")
                    
                    # å¯¹ HumanEval é¢˜ç›®ï¼Œä¿®æ”¹éœ€æ±‚è¯´æ˜Žæ¥æŒ‡å¯¼ Coder
                    humaneval_requirement = (
                        requirement + "\n\n"
                        "ã€é‡è¦è¯´æ˜Žã€‘è¿™æ˜¯ HumanEval ç¼–ç¨‹é¢˜ï¼Œè¯·åªç”Ÿæˆè¦æ±‚çš„å‡½æ•°å®žçŽ°ã€‚\n"
                        "- ä¸éœ€è¦ main() å‡½æ•°\n"
                        "- ä¸éœ€è¦æµ‹è¯•ä»£ç \n"
                        "- åªç”Ÿæˆå‡½æ•°å®šä¹‰å’Œå¿…è¦çš„è¾…åŠ©å‡½æ•°\n"
                        "- ç¡®ä¿å‡½æ•°åç§°ä¸Žé¢˜ç›®è¦æ±‚å®Œå…¨ä¸€è‡´"
                    )
                    
                    # è°ƒç”¨å®Œæ•´çš„ agent å·¥ä½œæµ
                    agent_result = solve(
                        requirement=humaneval_requirement,
                        model_name=model_name,
                        max_iterations=max_iterations,
                        enable_plan=False,  # HumanEval å•å‡½æ•°ï¼Œä¸éœ€è¦è§„åˆ’
                        project_name=task_id.replace('/', '_'),
                    )
                    
                    # æå–ç”Ÿæˆçš„ä»£ç 
                    generated_files = agent_result.get("generated_code", {})
                    if not generated_files:
                        typer.echo(f"   âš ï¸  Agent æœªç”Ÿæˆä»»ä½•ä»£ç ")
                        continue
                    
                    # æ‰¾åˆ°ä¸»å‡½æ•°æ‰€åœ¨çš„æ–‡ä»¶ï¼ˆä¼˜å…ˆçº§ï¼šåŒ…å« entry_point â†’ main.py â†’ ç¬¬ä¸€ä¸ª py æ–‡ä»¶ï¼‰
                    completion = ""
                    
                    # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæ‰¾åŒ…å«ç›®æ ‡å‡½æ•°çš„æ–‡ä»¶
                    for filename, code_content in generated_files.items():
                        if filename.endswith(".py") and filename != "__init__.py":
                            if f"def {entry_point}" in code_content:
                                completion = code_content
                                break
                    
                    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šæ‰¾ main.py
                    if not completion:
                        if "main.py" in generated_files:
                            main_content = generated_files["main.py"]
                            # æ£€æŸ¥ main.py ä¸­æ˜¯å¦æœ‰ç›®æ ‡å‡½æ•°ï¼ˆå¯èƒ½åœ¨ main.py ä¸­å®šä¹‰ï¼‰
                            if f"def {entry_point}" in main_content:
                                completion = main_content
                    
                    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªéž __init__.py çš„æ–‡ä»¶
                    if not completion:
                        for filename in sorted(generated_files.keys()):
                            if filename.endswith(".py") and filename != "__init__.py":
                                completion = generated_files[filename]
                                break
                    
                    if not completion:
                        typer.echo(f"   âš ï¸  æ— æ³•ä»Žç”Ÿæˆçš„æ–‡ä»¶ä¸­æå–ä»£ç ")
                        continue
                    
                    # æå–çº¯å‡½æ•°å®šä¹‰ï¼ˆåŽ»æŽ‰æµ‹è¯•ä»£ç ç­‰ï¼‰
                    completion = _extract_python_code(completion)
                    last_completion = completion
                    
                    # ç”¨ HumanEval å®˜æ–¹æµ‹è¯•éªŒè¯
                    test_code = problem["prompt"] + "\n" + completion + "\n" + problem["test"] + "\n" + f"check({entry_point})"
                    
                    passed = False
                    error_msg = "æœªçŸ¥é”™è¯¯"
                    try:
                        exec_globals = {}
                        exec(test_code, exec_globals)
                        passed = True
                        error_msg = "é€šè¿‡"
                    except AssertionError as e:
                        error_msg = f"AssertionError: {str(e)}"
                    except SyntaxError as e:
                        error_msg = f"SyntaxError: {str(e)}"
                        debug_file = pathlib.Path(f"results/.debug_{task_id.replace('/', '_')}.py")
                        debug_file.write_text(test_code, encoding="utf-8")
                        error_msg += f" (è°ƒè¯•æ–‡ä»¶: {debug_file})"
                    except Exception as e:
                        error_msg = f"{type(e).__name__}: {str(e)}"
                    
                    result = {"passed": passed, "result": error_msg}
                    
                    # ä¿å­˜æ ·æœ¬
                    record = {
                        "task_id": task_id,
                        "completion": completion,
                        "completion_id": f"agent-{sample_idx}",
                        "agent_tests_passed": agent_result.get("file_tests", {}),
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
                    all_samples.append(record)
                    
                    # æ£€æŸ¥æ˜¯å¦é€šè¿‡
                    if result.get("passed", False):
                        task_passed = True
                        if num_samples == 1:
                            typer.echo(f"\n   âœ… HumanEval å®˜æ–¹æµ‹è¯•é€šè¿‡")
                        else:
                            typer.echo(f"\n   âœ… ç‰ˆæœ¬ {sample_idx+1} HumanEval å®˜æ–¹æµ‹è¯•é€šè¿‡")
                        break
                    else:
                        error_msg = result.get("result", "æœªçŸ¥é”™è¯¯")
                        if num_samples > 1:
                            typer.echo(f"\n   â³ ç‰ˆæœ¬ {sample_idx+1} HumanEval å®˜æ–¹æµ‹è¯•å¤±è´¥: {error_msg[:100]}")
                        last_error = error_msg
                
                if task_passed:
                    passed_count += 1
                elif not task_passed and num_samples == 1:
                    typer.echo(f"\n   âŒ HumanEval å®˜æ–¹æµ‹è¯•å¤±è´¥")
                    typer.echo(f"      åŽŸå› : {last_error[:300]}")
                    typer.echo(f"      ç”Ÿæˆä»£ç é¢„è§ˆ:")
                    code_lines = last_completion.split('\n')[:10]
                    for line in code_lines:
                        typer.echo(f"        {line}")
                    total_lines = len(last_completion.split('\n'))
                    if total_lines > 10:
                        typer.echo(f"        ... (å…± {total_lines} è¡Œ)")
                elif not task_passed:
                    typer.echo(f"\n   âŒ æ‰€æœ‰ {num_samples} ä¸ªç‰ˆæœ¬éƒ½æœªé€šè¿‡ HumanEval å®˜æ–¹æµ‹è¯•")
                    typer.echo(f"      æœ€åŽé”™è¯¯: {last_error[:300]}")
                
                completed += 1
                
            except Exception as e:
                typer.echo(f"\n   âŒ Agent æ‰§è¡Œå¤±è´¥: {str(e)}")
                import traceback
                typer.echo(f"      è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    typer.echo(f"\n{'='*70}")
    typer.echo(f"ðŸ“Š è¯„ä¼°å®Œæˆ")
    typer.echo(f"{'='*70}")
    typer.echo(f"   â€¢ è¯„ä¼°é—®é¢˜æ•°: {completed}")
    typer.echo(f"   â€¢ é€šè¿‡é—®é¢˜æ•°: {passed_count}")
    typer.echo(f"   â€¢ é€šè¿‡çŽ‡: {passed_count/completed*100:.1f}%")
    typer.echo(f"   â€¢ æ ·æœ¬æ–‡ä»¶: {samples_file}")
    typer.echo(f"{'='*70}\n")


@app.callback(invoke_without_command=True)
def main(ctx: typer.Context) -> None:
    """HumanEval è¯„ä¼°å·¥å…·"""
    if ctx.invoked_subcommand is None:
        typer.echo(ctx.get_help())


if __name__ == "__main__":
    app()
